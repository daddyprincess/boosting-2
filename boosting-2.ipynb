{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9186c084-9cac-46f1-89a5-8ad9e01224da",
   "metadata": {},
   "source": [
    "## Q1. What is Gradient Boosting Regression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ead51d-0470-4734-ba53-da660e94db50",
   "metadata": {},
   "outputs": [],
   "source": [
    "Gradient Boosting Regression, often referred to as Gradient Boosting Machines (GBM) for regression, is a powerful machine \n",
    "learning technique used for regression tasks. It is an ensemble learning method that combines the predictions of multiple\n",
    "weak regression models (typically decision trees) to create a strong regression model that can accurately predict continuous\n",
    "numeric values.\n",
    "\n",
    "Here's how Gradient Boosting Regression works:\n",
    "\n",
    "1.Initialization: Gradient Boosting Regression starts with an initial prediction for all data points, which is often set to\n",
    "the mean of the target variable. This initial prediction serves as the starting point for the ensemble.\n",
    "\n",
    "2.Iteration (T): The algorithm proceeds through a series of iterations, where T is a hyperparameter set by the user. During\n",
    "each iteration, a weak regression model (typically a decision tree with limited depth) is trained on the residuals of the\n",
    "previous predictions. Residuals are the differences between the true target values and the current ensemble's predictions.\n",
    "\n",
    "3.Prediction Update: The predictions made by the current weak learner are added to the ensemble's predictions, but they are\n",
    "scaled by a learning rate (also a hyperparameter). The learning rate controls the contribution of each weak learner to the\n",
    "final prediction. Smaller learning rates make the model more robust but require more weak learners.\n",
    "\n",
    "4.Residual Calculation: After each iteration, the residuals are recalculated as the differences between the true target \n",
    "values and the current ensemble's predictions. The goal is to fit the weak learners to the errors made by the previous\n",
    "ensemble.\n",
    "\n",
    "5.Model Weight Calculation: Each weak learner is assigned a weight that determines its influence on the final prediction.\n",
    "The weight is computed using a gradient descent-like approach that minimizes the loss function (usually mean squared error\n",
    "for regression tasks). Models that reduce the loss more receive higher weights.\n",
    "\n",
    "6.Final Prediction: After T iterations, the ensemble combines the predictions of all the weak learners to make a final\n",
    "regression prediction for each data point. This final prediction is typically the sum of the predictions made by each weak\n",
    "learner, scaled by their respective weights.\n",
    "\n",
    "The key idea behind Gradient Boosting Regression is that it iteratively improves the model's predictions by focusing on the\n",
    "errors made in previous iterations. Weak learners are added to the ensemble to capture the nuances in the data that are not \n",
    "well-handled by the existing ensemble. This iterative and adaptive approach often leads to accurate and robust regression\n",
    "models.\n",
    "\n",
    "Gradient Boosting Regression can be further extended with variations like XGBoost, LightGBM, and CatBoost, which optimize\n",
    "the algorithm for efficiency and predictive performance. These variations offer additional features such as regularization,\n",
    "parallel processing, and better handling of categorical variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a4cf4f-dec4-47e5-9344-424aef379ebb",
   "metadata": {},
   "source": [
    "## Q2. Implement a simple gradient boosting algorithm from scratch using Python and NumPy. Use a simple regression problem as an example and train the model on a small dataset. Evaluate the model's performance using metrics such as mean squared error and R-squared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d3283e-afd7-41ae-94fe-a343b2928189",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate a simple dataset\n",
    "np.random.seed(0)\n",
    "X = np.sort(5 * np.random.rand(80, 1), axis=0)\n",
    "y = np.sin(X).ravel() + np.random.randn(80) * 0.1\n",
    "\n",
    "# Define the number of iterations (weak learners)\n",
    "n_estimators = 100\n",
    "\n",
    "# Define the learning rate\n",
    "learning_rate = 0.1\n",
    "\n",
    "# Initialize the predictions with the mean of the target variable\n",
    "predictions = np.full_like(y, np.mean(y))\n",
    "\n",
    "# Implement gradient boosting\n",
    "for i in range(n_estimators):\n",
    "    # Calculate the residuals (errors)\n",
    "    residuals = y - predictions\n",
    "    \n",
    "    # Fit a decision tree regressor to the residuals\n",
    "    tree = DecisionTreeRegressor(max_depth=2)\n",
    "    tree.fit(X, residuals)\n",
    "    \n",
    "    # Make predictions using the decision tree\n",
    "    tree_predictions = tree.predict(X)\n",
    "    \n",
    "    # Update the predictions with a scaled version of the tree predictions\n",
    "    predictions += learning_rate * tree_predictions\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y, predictions)\n",
    "r2 = r2_score(y, predictions)\n",
    "\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "print(f\"R-squared: {r2}\")\n",
    "\n",
    "# Plot the true data and the predicted values\n",
    "plt.scatter(X, y, s=20, edgecolor=\"black\", c=\"darkorange\", label=\"data\")\n",
    "plt.plot(X, predictions, color=\"cornflowerblue\", label=\"prediction\")\n",
    "plt.xlabel(\"data\")\n",
    "plt.ylabel(\"target\")\n",
    "plt.title(\"Gradient Boosting Regression\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "296345fb-6bd1-4497-877a-ce7e2e769e6c",
   "metadata": {},
   "source": [
    "## Q3. Experiment with different hyperparameters such as learning rate, number of trees, and tree depth to optimise the performance of the model. Use grid search or random search to find the best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb86b1e-ceb5-4979-92c0-b85625a57625",
   "metadata": {},
   "outputs": [],
   "source": [
    "Optimizing hyperparameters for a machine learning model, such as a random forest, is crucial to achieve the best possible \n",
    "performance. You can use grid search or random search techniques to find the optimal hyperparameters. Here's how you can \n",
    "do it:\n",
    "\n",
    "Import Libraries:\n",
    "\n",
    "Start by importing the necessary libraries, including the ones for your random forest model and hyperparameter tuning.\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "\n",
    "\n",
    "Define Hyperparameter Grids:\n",
    "\n",
    "Define a grid of hyperparameters you want to search through. For example:\n",
    "    \n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ['auto', 'sqrt', 'log2'],\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "\n",
    "\n",
    "Alternatively, for random search, define a distribution of hyperparameters:\n",
    "    \n",
    "param_dist = {\n",
    "    'n_estimators': [50, 100, 150, 200],\n",
    "    'max_depth': [None, 10, 20, 30, 40, 50],\n",
    "    'min_samples_split': [2, 5, 10, 15],\n",
    "    'min_samples_leaf': [1, 2, 4, 8],\n",
    "    'max_features': ['auto', 'sqrt', 'log2'],\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "\n",
    "\n",
    "Instantiate the Random Forest Model:\n",
    "\n",
    "Create an instance of the random forest classifier:\n",
    "    \n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "\n",
    "Grid Search or Random Search:\n",
    "\n",
    "Perform grid search or random search to find the best hyperparameters:\n",
    "\n",
    "Grid Search:\n",
    "    \n",
    "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5, n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "Random Search:\n",
    "    \n",
    "random_search = RandomizedSearchCV(estimator=rf, param_distributions=param_dist, n_iter=100, cv=5, n_jobs=-1)\n",
    "random_search.fit(X_train, y_train)\n",
    "best_params = random_search.best_params_\n",
    "\n",
    "Evaluate the Model:\n",
    "\n",
    "Train a random forest model using the best hyperparameters found and evaluate its performance:\n",
    "    \n",
    "best_rf = RandomForestClassifier(**best_params)\n",
    "best_rf.fit(X_train, y_train)\n",
    "accuracy = best_rf.score(X_test, y_test)\n",
    "\n",
    "\n",
    "Further Tuning:\n",
    "\n",
    "Depending on the results, you may need to further fine-tune the model by exploring additional hyperparameter combinations \n",
    "or adjusting other aspects of the model, such as feature selection, data preprocessing, or ensemble methods.\n",
    "\n",
    "Cross-Validation:\n",
    "\n",
    "Always use cross-validation (as demonstrated by the cv parameter in GridSearchCV or RandomizedSearchCV) to ensure that your\n",
    "hyperparameter optimization results are robust and not overfitting to your specific dataset.\n",
    "\n",
    "Repeat as Necessary:\n",
    "\n",
    "You can repeat the above steps until you find the hyperparameters that give you the best model performance on your specific\n",
    "problem. Be cautious not to overfit to your validation data during this process.\n",
    "\n",
    "Remember that hyperparameter tuning can be computationally expensive, so it's essential to strike a balance between finding\n",
    "the best hyperparameters and the available computing resources."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c108aa-43d1-4e42-b612-d4d0b9d58c08",
   "metadata": {},
   "source": [
    "## Q4. What is a weak learner in Gradient Boosting?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39360009-637d-401b-bc79-3c949e65b4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "In Gradient Boosting, a weak learner, also known as a base learner or base model, refers to a simple, often underperforming\n",
    "machine learning model that is used as a building block in the ensemble learning process. Gradient Boosting is an ensemble\n",
    "learning technique that combines multiple weak learners to create a strong predictive model.\n",
    "\n",
    "The term \"weak learner\" does not imply that the individual model is necessarily poor at learning or predicting; instead, it\n",
    "typically means that the model's performance is slightly better than random chance. Weak learners are often characterized by\n",
    "their simplicity and limited predictive power when compared to more complex models. Common examples of weak learners include\n",
    "decision stumps (decision trees with only one split), shallow decision trees (trees with a small depth), and linear models\n",
    "(like linear regression).\n",
    "\n",
    "The strength of Gradient Boosting lies in its ability to sequentially train weak learners, each one focusing on the mistakes\n",
    "or residuals of the previous learners. By iteratively adding these weak learners and giving more weight to the data points\n",
    "that the previous learners misclassified or predicted incorrectly, Gradient Boosting builds a strong ensemble model that\n",
    "can capture complex patterns and relationships within the data.\n",
    "\n",
    "The most popular form of Gradient Boosting, called Gradient Boosting Trees (or Gradient Boosting Machines, GBMs), builds an\n",
    "ensemble of decision trees as weak learners. These trees are usually shallow (low depth) and have a limited number of leaves.\n",
    "The combination of these simple trees, with each one refining the predictions based on the errors of the previous trees,\n",
    "leads to a powerful predictive model.\n",
    "\n",
    "In summary, a weak learner in Gradient Boosting is a simple, modestly performing model that is part of an ensemble learning \n",
    "process where the combination of multiple weak learners leads to a strong, high-performing predictive model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f200b1fe-2d8c-4782-bc35-c28c5f70ae37",
   "metadata": {},
   "source": [
    "## Q5. What is the intuition behind the Gradient Boosting algorithm?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7a5ea6-6eea-46e2-9b63-f5de1c52f7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "The intuition behind the Gradient Boosting algorithm can be summarized as follows:\n",
    "\n",
    "1.Ensemble Learning:\n",
    "\n",
    "    ~Gradient Boosting is an ensemble learning technique, which means it combines the predictions of multiple weak learners\n",
    "    (often decision trees) to create a strong, accurate predictive model. The idea is that by combining the outputs of \n",
    "    several models, the ensemble can correct the weaknesses of individual models and make more accurate predictions.\n",
    "\n",
    "2.Sequential Training:\n",
    "\n",
    "    ~Gradient Boosting trains these weak learners sequentially, rather than in parallel. It starts with an initial weak\n",
    "    learner and then builds subsequent learners to correct the errors or residuals made by the previous ones. This stepwise\n",
    "    process is crucial to the algorithm's success.\n",
    "\n",
    "3.Focus on Mistakes:\n",
    "\n",
    "    ~Each new learner in the sequence focuses on the mistakes made by the ensemble of learners built so far. It identifies\n",
    "    the data points that were misclassified or had large prediction errors and assigns more weight to them in the training \n",
    "    process. This \"focus on mistakes\" allows the algorithm to progressively improve its predictions.\n",
    "\n",
    "4.Gradient Descent Optimization:\n",
    "\n",
    "    ~The term \"Gradient\" in Gradient Boosting comes from the use of gradient descent optimization to minimize the loss\n",
    "    function. The loss function measures how far off the model's predictions are from the actual target values. By\n",
    "    iteratively adjusting the model's predictions in the direction that reduces the loss (negative gradient), Gradient\n",
    "    Boosting minimizes this error, making the model progressively better.\n",
    "\n",
    "5.Combining Weak Learners:\n",
    "\n",
    "    ~The predictions of the weak learners are combined by assigning each learner a weight in the final prediction. Learners \n",
    "    that perform well are given higher weights, while those with poorer performance receive lower weights. This combination \n",
    "    of multiple models, each specialized in correcting specific errors, results in a more accurate and robust final model.\n",
    "\n",
    "6.Regularization and Shallow Trees:\n",
    "\n",
    "    ~To prevent overfitting, Gradient Boosting typically uses shallow decision trees as weak learners, which have limited \n",
    "    depth and a small number of leaves. Regularization techniques like tree pruning and limiting the tree depth help control\n",
    "    model complexity.\n",
    "\n",
    "In essence, the intuition behind Gradient Boosting is to iteratively build a strong predictive model by focusing on and\n",
    "correcting the errors made by the ensemble of weak learners. By continuously improving the model's predictions in the\n",
    "direction of reducing the loss function, Gradient Boosting creates a highly accurate and robust predictive model that can\n",
    "handle complex relationships within the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80801864-0d1b-47ab-acf2-6687fad186de",
   "metadata": {},
   "source": [
    "## Q6. How does Gradient Boosting algorithm build an ensemble of weak learners?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c73071-4475-4e62-acfd-d57cce67e316",
   "metadata": {},
   "outputs": [],
   "source": [
    "The Gradient Boosting algorithm builds an ensemble of weak learners in a sequential manner. Here's a step-by-step\n",
    "explanation of how Gradient Boosting constructs this ensemble:\n",
    "\n",
    "1.Initialization:\n",
    "\n",
    "    ~Gradient Boosting starts with an initial prediction, often referred to as the \"initial guess\" or \"constant model.\" \n",
    "    This can be as simple as the mean of the target values for regression problems or the most frequent class for\n",
    "    classification problems. The first weak learner is then trained to correct the errors in this initial prediction.\n",
    "\n",
    "Sequential Training:\n",
    "\n",
    "Gradient Boosting trains the weak learners sequentially. For each iteration (or boosting round), the algorithm does the\n",
    "following:\n",
    "\n",
    "a. Compute Residuals:\n",
    "\n",
    "    ~Calculate the residuals (the differences between the actual target values and the current ensemble's predictions) for\n",
    "    each data point in the training set. These residuals represent the errors that the current ensemble of weak learners has\n",
    "    made.\n",
    "\n",
    "b. Train a Weak Learner:\n",
    "\n",
    "    ~Fit a new weak learner (usually a decision tree) to the residuals obtained in step (a). This new weak learner is\n",
    "    trained to predict the residuals, effectively learning how to correct the errors made by the current ensemble.\n",
    "\n",
    "c. Update Ensemble Predictions:\n",
    "\n",
    "    ~Combine the predictions of the newly trained weak learner with the predictions of the previous weak learners. The\n",
    "    contributions of each learner are weighted, and their weighted sum is added to the current ensemble's predictions.\n",
    "\n",
    "d. Update Weights:\n",
    "\n",
    "    ~Adjust the weights assigned to the data points in the training set. Data points that were misclassified or had large\n",
    "    residuals are given higher weights, while correctly classified points are given lower weights. This weighting process\n",
    "    ensures that the next weak learner focuses on the data points where the ensemble's predictions are still inaccurate.\n",
    "\n",
    "e. Repeat:\n",
    "\n",
    "    ~Repeat steps (a) through (d) for a specified number of iterations or until a stopping criterion is met. The number of\n",
    "    iterations is a hyperparameter that you can tune.\n",
    "\n",
    "2.Final Prediction:\n",
    "\n",
    "    ~After all the boosting rounds are completed, the final prediction is obtained by summing the predictions of all weak \n",
    "    learners, each scaled by its respective weight. This final prediction is often a much more accurate and robust model\n",
    "    than any of the individual weak learners.\n",
    "\n",
    "In summary, Gradient Boosting builds an ensemble of weak learners by iteratively training new weak learners to correct the\n",
    "errors made by the current ensemble. This sequential process focuses on improving the predictions on data points where the \n",
    "ensemble is still performing poorly, ultimately leading to a strong predictive model. The combination of many weak learners,\n",
    "each specialized in correcting specific errors, results in a powerful ensemble that can capture complex relationships in the\n",
    "data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b75e12-b3d8-49f6-8ce6-5e340f4d5c54",
   "metadata": {},
   "source": [
    "## Q7. What are the steps involved in constructing the mathematical intuition of Gradient Boosting algorithm?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27fa2c78-3ca7-429c-82df-b4fdb1b5d379",
   "metadata": {},
   "outputs": [],
   "source": [
    "Constructing the mathematical intuition of the Gradient Boosting algorithm involves understanding the core mathematical \n",
    "concepts and operations that underlie the algorithm. Here are the key steps involved in building the mathematical intuition\n",
    "for Gradient Boosting:\n",
    "\n",
    "1.Initialize the Ensemble:\n",
    "\n",
    "    ~Start with an initial prediction, often denoted as F0(x). This initial prediction can be a simple constant value, such \n",
    "     as the mean of the target values for regression or the log-odds of class probabilities for classification.\n",
    "        \n",
    "2.Compute Residuals:\n",
    "\n",
    "    ~Calculate the residuals for each data point in the training set. The residuals represent the errors between the actual\n",
    "    target values (y) and the current ensemble's prediction (Ft(x)).\n",
    "\n",
    "    ~Residuals (ri) for each data point i are computed as: ri=yi−Ft(xi), where yi is the true target value for data point i,\n",
    "     and Ft(xi) is the prediction of the ensemble up to iteration t.\n",
    "\n",
    "3.Train a Weak Learner:\n",
    "\n",
    "    ~Fit a new weak learner (usually a decision tree) to the residuals. This weak learner is trained to predict the residuals\n",
    "    (ri) rather than the target values themselves.\n",
    "\n",
    "    ~The weak learner learns a function ht(x) that approximates the residuals, i.e., ht(x)≈ri.\n",
    "\n",
    "4.Update Ensemble Predictions:\n",
    "\n",
    "    ~Combine the predictions of the newly trained weak learner (ht(x)) with the predictions of the current ensemble (Ft(x)). \n",
    "     This creates an updated prediction for the ensemble at iteration t+1.\n",
    "\n",
    "    ~The updated prediction at iteration t+1 is given by: Ft+1(x)=Ft(x)+η⋅h t(x), where η (the learning rate) is a\n",
    "    hyperparameter that controls the step size for updating the ensemble's predictions.\n",
    "\n",
    "    ~The learning rate (η) is a regularization parameter that prevents overfitting and controls the contribution of each\n",
    "    weak learner to the ensemble.\n",
    "\n",
    "5.Update Weights:\n",
    "\n",
    "    ~Adjust the weights assigned to the data points in the training set. Data points that were misclassified or had large \n",
    "    residuals are given higher weights, while correctly classified points are given lower weights. This weighting process\n",
    "    ensures that the next weak learner focuses on the data points where the ensemble's predictions are still inaccurate.\n",
    "\n",
    "    ~The updated weights for each data point are used when training the weak learner at the next iteration.\n",
    "\n",
    "6.Repeat:\n",
    "\n",
    "    ~Repeat steps 2 to 5 for a specified number of iterations (boosting rounds) or until a stopping criterion is met. The \n",
    "    number of iterations is a hyperparameter that you can tune.\n",
    "    \n",
    "7.Final Prediction:\n",
    "\n",
    "    ~The final prediction of the Gradient Boosting ensemble is the sum of the predictions of all weak learners, each scaled\n",
    "    by its respective weight:\n",
    "\n",
    "            F(x)=F0(x)+η⋅h1(x)+η⋅h2(x)+…+η⋅hT(x)\n",
    "\n",
    "where F(x) is the final prediction, η is the learning rate,ℎt(x) is the prediction of the weak learner at iteration t, and T\n",
    "is the total number of iterations.\n",
    "\n",
    "In summary, the mathematical intuition of Gradient Boosting revolves around sequentially training weak learners to \n",
    "approximate the residuals of the current ensemble, updating the ensemble's predictions, and adjusting the data point weights\n",
    "to focus on misclassified or poorly predicted samples. The final prediction is a weighted sum of the predictions of all weak\n",
    "learners, resulting in a powerful and accurate predictive model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
